{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bd1766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36e9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Is it okay to initialize all the weights to the same value as long as that value is selected\n",
    "randomly using He initialization?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2a471",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initializing all the weights to the same value, even if randomly selected using He initialization, is not recommended. While He initialization helps with the proper initialization of weights to prevent the vanishing/exploding gradients problem, initializing all the weights to the same value would result in symmetry across the network, hindering the learning process. It is essential to introduce some level of diversity in weight initialization to break the symmetry and allow the network to learn unique features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e79fc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e9148d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is it okay to initialize the bias terms to 0?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd5fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Initializing the bias terms to 0 is generally acceptable. Bias terms act as offsets and are often initialized to 0 since they do not need any specific initialization pattern. During training, the network will learn the appropriate bias values based on the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4758fd6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three advantages of the ELU activation function over ReLU.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42caa262",
   "metadata": {},
   "outputs": [],
   "source": [
    "Three advantages of the Exponential Linear Unit (ELU) activation function over Rectified Linear Unit (ReLU) are:\n",
    "   - ELU allows negative values, which can help capture information that may be lost with ReLU's thresholding at 0.\n",
    "   - ELU has a smooth gradient for both positive and negative inputs, which can improve the training process and gradient flow.\n",
    "   - ELU reduces the likelihood of dead neurons, where neurons become non-responsive during training. This can improve model performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2327eea6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a3a86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. In which cases would you want to use each of the following activation functions: ELU, leaky\n",
    "ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2137da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "The choice of activation function depends on the specific problem and network architecture:\n",
    "   - ELU: Suitable when there is a concern about negative inputs and dead neurons.\n",
    "   - Leaky ReLU (and its variants): Useful when you want to introduce a small gradient for negative inputs, preventing dead neurons.\n",
    "   - ReLU: Appropriate for most cases as a default choice, especially when the network is deep and the vanishing gradient problem can occur.\n",
    "   - Tanh: Useful for cases where inputs are normalized, and negative values are significant.\n",
    "   - Logistic (Sigmoid): Appropriate for binary classification problems as the output layer activation function.\n",
    "   - Softmax: Ideal for multi-class classification tasks as the output layer activation function, providing class probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6e047a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e55ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999)\n",
    "when using a MomentumOptimizer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465d511f",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the momentum hyperparameter is set too close to 1 (e.g., 0.99999) in the MomentumOptimizer, it can lead to slow convergence or instability in training. With such high momentum, the optimizer will accumulate historical gradients for a more extended period, causing the updates to be dominated by past gradients. This can result in slow convergence and potential oscillations around the optimal solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7e5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3b34c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Name three ways you can produce a sparse model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd84fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Three ways to produce a sparse model are:\n",
    "   - L1 Regularization (Lasso): Introduces a penalty term based on the absolute values of weights, encouraging sparsity by driving some weights to exactly zero.\n",
    "   - Dropout: Randomly sets a fraction of input units to zero during training, forcing the network to learn redundant representations and reducing reliance on specific weights.\n",
    "   - Pruning: After training, weights below a certain threshold or with insignificant contributions can be pruned (set to zero) to create a sparse model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930af1e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb92e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on\n",
    "new instances)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1375b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout can slow down training as it introduces randomness and requires the network to learn redundant representations. However, during inference (making predictions on new instances), dropout is typically turned off, and therefore, inference is not slowed down. Dropout can actually help improve generalization and prevent overfitting by reducing reliance on specific weights and encouraging the network to learn more robust representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0cd26f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968551f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a97d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0515f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b1861a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62eec03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf5bbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bccd2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c96538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28741de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57c4d4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bf9da4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747b93d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab25f37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941064ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be534860",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1eb2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70aaec5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
