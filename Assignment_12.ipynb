{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072f4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe6ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. How does unsqueeze help us to solve certain broadcasting problems?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbab7a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "The unsqueeze function helps us solve certain broadcasting problems by adding a new dimension to a tensor. When broadcasting requires dimensions to match for elementwise operations, unsqueeze can be used to expand the tensor along the specified dimension, allowing proper alignment with the other tensor. It helps in adjusting the shapes of tensors to enable compatible broadcasting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f44d0dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d32c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. How can we use indexing to do the same operation as unsqueeze?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e618499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n",
      "[[1 2 3]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3])\n",
    "b = a[:, None]  # Using indexing with None\n",
    "c = a[np.newaxis, :]  # Using indexing with np.newaxis\n",
    "\n",
    "print(b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d61680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596c854",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How do we show the actual contents of the memory used for a tensor?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df693067",
   "metadata": {},
   "outputs": [],
   "source": [
    "To show the actual contents of the memory used for a tensor, you can access the underlying data by calling the .numpy() method (for PyTorch tensors) or the .tolist() method (for TensorFlow tensors) on the tensor object. This will return a NumPy array or a Python list containing the tensor's data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74af184e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d23d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. When adding a vector of size 3 to a matrix of size 3×3, are the elements of the vector added\n",
    "to each row or each column of the matrix? (Be sure to check your answer by running this\n",
    "code in a notebook.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "When adding a vector of size 3 to a matrix of size 3x3, the elements of the vector are added to each column of the matrix. Broadcasting rules dictate that when adding tensors with different shapes, the smaller tensor is expanded to match the shape of the larger tensor. In this case, the vector is broadcasted along the second dimension (columns) of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83892a72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5  7  9]\n",
      " [ 8 10 12]\n",
      " [11 13 15]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix = np.array([[4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "result = matrix + vector\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba332470",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Do broadcasting and expand_as result in increased memory use? Why or why not?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe701de",
   "metadata": {},
   "outputs": [],
   "source": [
    "Broadcasting and expand_as do not result in increased memory use. They do not create additional copies of the tensor data. Instead, they allow efficient computation by adjusting the tensor's shape and manipulating the indexing mechanism to avoid unnecessary memory duplication. The expanded view of the tensor is used during computation without actually allocating additional memory for the expanded tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360822df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae59aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Implement matmul using Einstein summation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef954b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.37964213 0.87073231 0.87967245 1.14610443 0.91575191]\n",
      " [1.43531051 1.46566456 0.87257348 1.53638773 1.20553217]\n",
      " [1.28770516 1.04972445 0.83758174 1.081176   1.20259086]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.random.rand(3, 4)\n",
    "b = np.random.rand(4, 5)\n",
    "\n",
    "c = np.einsum('ij,jk->ik', a, b)\n",
    "\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d132580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a106e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What does a repeated index letter represent on the lefthand side of einsum?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d1d5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "A repeated index letter on the left-hand side of Einstein summation notation represents a summation over that index. It indicates that the corresponding dimensions will be contracted, resulting in a summation operation when calculating the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5741b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014d29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What are the three rules of Einstein summation notation? Why?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317d2949",
   "metadata": {},
   "outputs": [],
   "source": [
    "The three rules of Einstein summation notation are:\n",
    "\n",
    "Repeated indices: A repeated index implies summation over that index.\n",
    "\n",
    "Free indices: Free indices are those that appear exactly once on both sides of the expression and are not summed over. They represent the indices of the resulting output tensor.\n",
    "\n",
    "Einstein's convention: In Einstein summation notation, an index that appears once as a subscript (lower index) and once as a superscript (upper index) indicates a summation over that index.\n",
    "\n",
    "These rules are used to compactly represent and perform tensor operations without explicitly writing out the summation signs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a0766e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aee8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. What are the forward pass and backward pass of a neural network?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45976022",
   "metadata": {},
   "outputs": [],
   "source": [
    "The forward pass of a neural network refers to the process of propagating input data through the network's layers, computing the intermediate activations, and producing an output prediction. During the forward pass, each layer performs a linear transformation (weighted sum) followed by an activation function to produce the output for the subsequent layer.\n",
    "\n",
    "The backward pass, also known as backpropagation, is the process of calculating the gradients of the loss function with respect to the network's parameters. It involves propagating the error signal from the output layer to the input layer, using the chain rule of derivatives to compute the gradients layer by layer. These gradients are then used to update the parameters of the network during the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75233b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82971458",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Why do we need to store some of the activations calculated for intermediate layers in the\n",
    "forward pass?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da51bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Storing activations calculated for intermediate layers in the forward pass is necessary for backpropagation and gradient computation during the backward pass. The intermediate activations are required to calculate the gradients of the loss function with respect to the network parameters. They serve as inputs for the chain rule, allowing the calculation of gradients layer by layer. Without storing the activations, it would not be possible to propagate the gradients backward through the network and update the parameters effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab9763a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3f9b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "11. What is the downside of having activations with a standard deviation too far away from 1?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362961f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The downside of having activations with a standard deviation too far away from 1 is that it can lead to vanishing or exploding gradients during training. Vanishing gradients occur when the gradients become extremely small, making it challenging for the network to learn and update the lower layers. Exploding gradients, on the other hand, occur when the gradients become extremely large, causing unstable updates and difficulties in convergence.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6cfa5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a51709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "12. How can weight initialization help avoid this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2788ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight initialization can help avoid the problem of activations with a standard deviation too far away from 1. Proper weight initialization techniques, such as Xavier/Glorot initialization or He initialization, set the initial values of the weights in a way that helps stabilize the training process. These techniques take into account the number of input and output units of a layer to initialize the weights appropriately. By initializing the weights correctly, the activations are more likely to have an appropriate range of values, preventing them from being too small or too large, and improving the convergence and stability of the network during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2dd86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e6550",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ce3a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
