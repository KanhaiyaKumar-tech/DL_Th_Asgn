{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3551440",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc5e0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Explain the Activation Functions in your own language\n",
    "a) sigmoid\n",
    "b) tanh\n",
    "c) ReLU\n",
    "d) ELU\n",
    "e) LeakyReLU\n",
    "f) swish\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba05ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    " Activation Functions:\n",
    "a) Sigmoid: The sigmoid function maps the input values to a range between 0 and 1. It is useful for binary classification tasks as it provides a smooth, non-linear transformation. However, it suffers from the vanishing gradient problem and tends to saturate for large input values, making learning slow.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27fe3e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Tanh: The hyperbolic tangent (tanh) function also maps the input values to a range between -1 and 1. It is similar to the sigmoid function but centered around 0. Tanh addresses the saturation problem of the sigmoid function, but it still suffers from the vanishing gradient problem.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d9583",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) ReLU (Rectified Linear Unit): ReLU sets all negative input values to zero and keeps positive values unchanged. It is the most widely used activation function in deep learning due to its simplicity and efficiency. ReLU helps in alleviating the vanishing gradient problem and enables faster convergence during training. However, ReLU neurons can be prone to dying out if the learning rate is set too high.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e54d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d) ELU (Exponential Linear Unit): ELU is a variant of ReLU that introduces a slight negative slope for negative input values. This helps in reducing the dying ReLU problem by allowing the neurons to have non-zero gradients for negative inputs. ELU also provides smoothness for negative inputs, which can help in better generalization.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc13a3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "e) LeakyReLU: LeakyReLU is another variant of ReLU that introduces a small slope for negative input values instead of setting them to zero. This helps in addressing the dying ReLU problem and allows for the potential of negative inputs to contribute to the learning process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d1785c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "f) Swish: Swish is a newer activation function that applies a non-linear transformation to the input values. It combines the benefits of ReLU and sigmoid functions by being smooth, non-monotonic, and having the potential for faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1bbd9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04f2d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What happens when you increase or decrease the optimizer learning rate?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f028afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Learning Rate: Increasing or decreasing the optimizer learning rate affects the speed and quality of the model's convergence during training. A higher learning rate can cause the optimization algorithm to take larger steps in the parameter space, leading to faster convergence. However, if the learning rate is set too high, it may result in overshooting the optimal solution and cause instability or divergence. On the other hand, a lower learning rate allows for smaller steps, which can help in fine-tuning the model and reaching a better optimum, but it might require more iterations for convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef975ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e90e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What happens when you increase the number of internal hidden neurons?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf921ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Number of Hidden Neurons: Increasing the number of internal hidden neurons in a neural network provides the model with more capacity to learn complex patterns and represent the underlying data. With more neurons, the network can capture intricate relationships in the data, potentially improving the model's performance. However, increasing the number of hidden neurons also increases the model's complexity, leading to a higher risk of overfitting and increased computational cost during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0599f8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fca2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. What happens when you increase the size of batch computation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43bf552",
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch Size: Increasing the size of batch computation refers to using larger subsets of the training data during each iteration of model training. Larger batch sizes can lead to more stable gradients and better generalization due to the use of more data for parameter updates. However, using larger batch sizes also increases the memory requirements and computational cost per iteration. It can also slow down the training process since fewer iterations are performed on the entire training dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b3f5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc49ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Why we adopt regularization to avoid overfitting?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbef92fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "Regularization: Regularization is adopted in deep learning to avoid overfitting, which occurs when a model becomes too complex and starts to memorize the training data instead of learning the underlying patterns. Regularization techniques, such as L1 or L2 regularization, add a penalty term to the loss function that discourages the model from assigning high weights to individual features. This helps in preventing over-reliance on specific features and encourages more generalizable representations, leading to better performance on unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43fb45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e27dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are loss and cost functions in deep learning?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8f9085",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Loss and Cost Functions: In deep learning, the loss function measures the discrepancy between the predicted output of the model and the true target values. It quantifies the model's performance during training and serves as a guide for updating the model's parameters. The choice of loss function depends on the specific task, such as binary classification, multiclass classification, or regression.\n",
    "\n",
    "The cost function, also known as the objective function, represents the average loss over the entire training dataset. It is obtained by aggregating the individual losses from each training example. The goal of training a deep learning model is to minimize the cost function, which involves adjusting the model's parameters through optimization algorithms like gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00efa79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d762a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "7. What do ou mean by underfitting in neural networks?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c901f943",
   "metadata": {},
   "outputs": [],
   "source": [
    "Underfitting: Underfitting in neural networks refers to a situation where the model fails to capture the underlying patterns and performs poorly on both the training and test data. It occurs when the model is too simple or lacks the capacity to represent the complexity of the data. Underfitting can result from using a small number of neurons or layers in the network, limited training data, or inadequate training time. An underfit model often exhibits high bias and low variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdce938",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4038623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe54244",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Why we use Dropout in Neural Networks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c078120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dropout: Dropout is a regularization technique used in neural networks to prevent overfitting. It involves randomly disabling a proportion of the neurons during each training iteration, effectively dropping them out. By doing so, dropout helps in reducing the interdependence between neurons and encourages the network to learn more robust and generalizable representations. Dropout can also act as an ensemble method by creating multiple subnetworks that share parameters, resulting in improved model performance and better generalization.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3719e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97ef46c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5333aabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aba827e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a8428d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bfdf16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d90166",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dfae0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c72f38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
