{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb9139",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4224b4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Describe the structure of an artificial neuron. How is it similar to a biological neuron? What\n",
    "are its main components?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdf0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "The structure of an artificial neuron, also known as a perceptron, is inspired by the biological neuron but simplified. It consists of the following main components:\n",
    "\n",
    "- Inputs: Neurons receive input signals from other neurons or external sources. Each input is associated with a weight, which determines its importance or contribution to the neuron's output.\n",
    "- Weights: Each input has an associated weight that represents the strength or significance of that input. The weights are adjustable parameters that are learned during the training process.\n",
    "- Summation Function: The inputs are multiplied by their corresponding weights, and the weighted values are summed together.\n",
    "- Activation Function: The sum obtained from the weighted inputs is passed through an activation function. The activation function introduces non-linearity and determines the output of the neuron based on the input sum.\n",
    "- Bias: A bias term is often included in the neuron's computation. It provides an additional adjustable parameter that helps shift the activation function's threshold and allows for better flexibility in the decision-making process.\n",
    "- Output: The output of the neuron is the result of the activation function applied to the sum of the weighted inputs.\n",
    "\n",
    "The structure of an artificial neuron is similar to a biological neuron in the sense that it receives inputs, processes them, and produces an output. However, it simplifies the biological neuron by disregarding complex mechanisms such as dendrites, axons, and the transmission of chemical signals.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2000ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35e362d",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. What are the different types of activation functions popularly used? Explain each of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc386ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "There are several types of activation functions used in artificial neural networks. Here are some popular ones:\n",
    "\n",
    "- Step Function: The step function produces a binary output. It returns 1 if the input is greater than or equal to zero and 0 otherwise. It has a sharp transition at the threshold value.\n",
    "- Sigmoid Function: The sigmoid function maps the input to a continuous output between 0 and 1. It has an S-shaped curve and is widely used due to its differentiability and smoothness. The logistic sigmoid and the hyperbolic tangent (tanh) functions are examples of sigmoid functions.\n",
    "- Rectified Linear Unit (ReLU): The ReLU function returns the input as it is if it is positive, and 0 otherwise. It is widely used in deep learning models due to its simplicity and effectiveness in handling the vanishing gradient problem.\n",
    "- Softmax Function: The softmax function is commonly used in multi-class classification problems. It takes a vector of real numbers as input and normalizes them to produce a probability distribution over the classes. It ensures that the sum of the output values is equal to 1.\n",
    "\n",
    "The choice of activation function depends on the problem at hand, the desired output range, and the characteristics of the data being processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbbd412",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f584901",
   "metadata": {},
   "outputs": [],
   "source": [
    "3.\n",
    "1. Explain, in details, Rosenblatt’s perceptron model. How can a set of data be classified using a\n",
    "simple perceptron?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35ba18",
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Rosenblatt's' perceptron model:\n",
    "Rosenblatt's' perceptron model is a binary classifier that learns to separate input data into two classes using a linear decision boundary. The model consists of a single artificial neuron with binary inputs and weights associated with each input.\n",
    "\n",
    "The classification process involves the following steps:\n",
    "1. Initialize the weights and bias to small random values.\n",
    "2. Present an input pattern to the perceptron.\n",
    "3. Compute the weighted sum of the inputs.\n",
    "4. Apply an activation function (usually a step function) to the weighted sum.\n",
    "5. Compare the output of the perceptron with the desired output.\n",
    "6. Adjust the weights and bias using a learning rule to minimize the error.\n",
    "7. Repeat steps 2-6 for all input patterns until convergence or a stopping criterion is met.\n",
    "\n",
    "The learning rule used in Rosenblatt's' perceptron model is based on the concept of gradient descent. It updates the weights and bias by adding or subtracting a small fraction of the input multiplied by the error. The learning process continues until the perceptron can classify the input patterns correctly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45722da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba8961e",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Use a simple perceptron with weights w 0 , w 1 , and w 2  as −1, 2, and 1, respectively, to classify\n",
    "data points (3, 4); (5, 2); (1, −3); (−8, −3); (−3, 0).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a4eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b) Using the given weights (w0 = -1, w1 = 2, w2 = 1) and assuming a step function as the activation function, we can classify the data points as follows:\n",
    "\n",
    "For the data point (3, 4):\n",
    "Input = w0 + w1 * x1 + w2 * x2 = -1 + 2 * 3 + 1 * 4 = 10\n",
    "Output = step(Input) = 1 (belongs to Class 1)\n",
    "\n",
    "For the data point (5, 2):\n",
    "Input = w0 + w1 * x1 + w2 * x2 = -1 + 2 * 5 + 1 * 2 = 11\n",
    "Output = step(Input) = 1 (belongs to Class 1)\n",
    "\n",
    "For the data point (1, -3):\n",
    "Input = w0 + w1 * x1 + w2 * x2 = -1 + 2 * 1 + 1 * (-3) = 0\n",
    "Output = step(Input) = 0 (belongs to Class 0)\n",
    "\n",
    "For the data point (-8, -3):\n",
    "Input = w0 + w1 * x1 + w2 * x2 = -1 + 2 * (-8) + 1 * (-3) = -22\n",
    "Output = step(Input) = 0 (belongs to Class 0)\n",
    "\n",
    "For the data point (-3, 0):\n",
    "Input = w0 + w1 * x1 + w2 * x2 = -1 + 2 * (-3) + 1 * 0 = -7\n",
    "Output = step(Input) = 0 (belongs to Class 0)\n",
    "\n",
    "The perceptron classifies the points (3, 4) and (5, 2) into Class 1 and the points (1, -3), (-8, -3), and (-3, 0) into Class 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086555ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e9a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Explain the basic structure of a multi-layer perceptron. Explain how it can solve the XOR\n",
    "problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc30594",
   "metadata": {},
   "outputs": [],
   "source": [
    "The basic structure of a multi-layer perceptron (MLP) consists of an input layer, one or more hidden layers, and an output layer. Each layer contains multiple artificial neurons (also called nodes or units), and the neurons within a layer are interconnected.\n",
    "\n",
    "In an MLP, each neuron in a layer is connected to all the neurons in the previous layer and the subsequent layer. Neurons within the same layer do not have direct connections between them. The connections between neurons have associated weights that are adjusted during the training process to optimize the network's performance.\n",
    "\n",
    "The input layer receives the input data, which is then propagated through the hidden layers. Each hidden layer applies an activation function to the weighted sum of its inputs and produces outputs. The outputs from the last hidden layer are passed to the output layer, where the final output of the network is generated.\n",
    "\n",
    "An MLP can solve the XOR problem, which is not solvable by a single-layer perceptron, by introducing a hidden layer with non-linear activation functions. The hidden layer allows the MLP to learn and represent complex non-linear relationships between the input variables, enabling it to accurately classify the XOR data. The XOR problem requires a decision boundary that is not linear, and the hidden layer helps the MLP learn and approximate this non-linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ea9060",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac1602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. What is artificial neural network (ANN)? Explain some of the salient highlights in the\n",
    "different architectural options for ANN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c072fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "An Artificial Neural Network (ANN) is a computational model inspired by the structure and function of biological neural networks. It consists of interconnected artificial neurons or nodes organized in layers. ANN processes information through these nodes and learns from data to perform tasks such as pattern recognition, classification, regression, and optimization.\n",
    "\n",
    "Salient highlights of different architectural options for ANN include:\n",
    "\n",
    "- Feedforward Neural Networks: These networks have information flowing in one direction, from the input layer to the output layer. They are the most basic type of ANN, and their architecture can be simple, consisting of only input and output layers, or complex, with multiple hidden layers in between.\n",
    "\n",
    "- Recurrent Neural Networks (RNNs): RNNs have feedback connections, allowing information to flow in loops. This architecture enables them to process sequential and temporal data by maintaining memory of previous inputs. RNNs are suitable for tasks like speech recognition, language modeling, and time series prediction.\n",
    "\n",
    "- Convolutional Neural Networks (CNNs): CNNs are designed for processing grid-like data, such as images or videos. They consist of convolutional layers that apply filters to input data, pooling layers for downsampling, and fully connected layers for classification. CNNs have shown remarkable performance in computer vision tasks.\n",
    "\n",
    "- Long Short-Term Memory (LSTM) Networks: LSTMs are a type of RNN architecture that can capture long-term dependencies in sequential data. They address the vanishing/exploding gradient problem and are well-suited for tasks involving sequential data with long-term dependencies, such as speech recognition, language translation, and sentiment analysis.\n",
    "\n",
    "- Autoencoders: Autoencoders are neural networks used for unsupervised learning and data compression. They consist of an encoder network that compresses the input data into a lower-dimensional representation (encoding), and a decoder network that reconstructs the original input from the encoded representation. Autoencoders find applications in areas like image denoising, anomaly detection, and dimensionality reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b1cea9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2c1443",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain the learning process of an ANN. Explain, with example, the challenge in assigning\n",
    "synaptic weights for the interconnection between neurons? How can this challenge be\n",
    "addressed?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "The learning process of an Artificial Neural Network involves adjusting the synaptic weights (connections between neurons) to minimize the difference between the network's output and the desired output. This process is typically performed in a supervised manner.\n",
    "\n",
    "During training, the network receives input data, which propagates through the network's layers, producing an output. The difference between the output and the desired output (the error) is calculated using a loss function. The error is then used to update the synaptic weights.\n",
    "\n",
    "The challenge in assigning synaptic weights lies in finding appropriate initial weights and optimizing them during training. If the initial weights are too small or too large, the network may struggle to converge to an optimal solution. Additionally, assigning weights randomly can result in slow convergence or getting stuck in suboptimal solutions.\n",
    "\n",
    "To address this challenge, techniques like weight initialization methods (e.g., Xavier or He initialization) are used to set the initial weights more effectively. Furthermore, optimization algorithms such as gradient descent and its variants (e.g., Adam, RMSprop) are employed to iteratively update the weights based on the gradients of the loss function, allowing the network to gradually improve its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735e4933",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c744b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "5. Explain, in details, the backpropagation algorithm. What are the limitations of this\n",
    "algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca986699",
   "metadata": {},
   "outputs": [],
   "source": [
    "The backpropagation algorithm is an efficient method for training multi-layer neural networks. It is a form of gradient-based optimization that adjusts the synaptic weights of the network to minimize the difference between the network's predicted output and the desired output.\n",
    "\n",
    "Here are the steps of the backpropagation algorithm:\n",
    "\n",
    "1. Forward Propagation: The input data is fed forward through the network, layer by layer, to compute the output. The activations and weighted sums of each neuron are stored for later use in the backward pass.\n",
    "\n",
    "2. Error Calculation: The error or loss is calculated by comparing the network's output with the desired output using a suitable loss function, such as mean squared error or cross-entropy.\n",
    "\n",
    "3. Backward Propagation: The error is propagated backward through the network, starting from the output layer. The gradients of the error with respect to the weighted sums of each neuron are computed using the chain rule of calculus.\n",
    "\n",
    "4. Weight Update: The gradients are used to update the weights of the connections between neurons. The weights are adjusted in the opposite direction of the gradient to minimize the error. The learning rate determines the step size of weight updates.\n",
    "\n",
    "5. Repeat: Steps 1 to 4 are repeated for multiple iterations (epochs) or until convergence, gradually reducing the error and improving the network's performance.\n",
    "\n",
    "Limitations of the backpropagation algorithm include:\n",
    "\n",
    "- It requires labeled training data, which may not always be available or may be expensive to obtain.\n",
    "- The algorithm can converge to local minima instead of the global minimum of the error surface.\n",
    "- It suffers from the vanishing or exploding gradient problem, particularly in deep networks with many layers. This can hinder learning or lead to slow convergence.\n",
    "- Backpropagation assumes that the network's error can be attributed solely to the weights, neglecting potential error sources in the model's architecture or assumptions.\n",
    "- The algorithm may be computationally expensive, especially for large-scale neural networks and complex tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094763f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3507bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "6. Describe, in details, the process of adjusting the interconnection weights in a multi-layer\n",
    "neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420a03bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "The process of adjusting the interconnection weights in a multi-layer neural network involves iteratively updating the weights using the backpropagation algorithm. Here's a detailed description:\n",
    "\n",
    "1. Initialize Weights: Randomly initialize the weights of the connections between neurons in the network. Proper weight initialization can help accelerate convergence.\n",
    "\n",
    "2. Forward Propagation: Pass an input through the network, calculating the activations and weighted sums of neurons in each layer, layer by layer, until the output layer is reached. Store the activations and weighted sums for each neuron.\n",
    "\n",
    "3. Error Calculation: Compare the network's output with the desired output using a suitable loss function. Calculate the error or loss, which quantifies the discrepancy between the predicted and desired outputs.\n",
    "\n",
    "4. Backward Propagation: Starting from the output layer, calculate the gradients of the error with respect to the weighted sums of each neuron using the chain rule. The gradients indicate how the error changes with small variations in the weighted sums.\n",
    "\n",
    "5. Weight Update: Use the gradients to update the weights of the connections between neurons. The weight update is typically performed using an optimization algorithm, such as gradient descent or one of its variants. The learning rate determines the step size of weight updates.\n",
    "\n",
    "6. Repeat: Repeat steps 2 to 5 for a batch or mini-batch of training examples. Iterate through multiple epochs, adjusting the weights based on the gradients to reduce the error and improve the network's performance.\n",
    "\n",
    "7. Continue until Convergence: Iterate through the training process until the network's performance converges, meaning the error reaches a satisfactory level or no longer significantly improves.\n",
    "\n",
    "The weights are adjusted based on the calculated gradients, which reflect the contribution of each weight to the overall error. By iteratively updating the weights, the network gradually optimizes its parameters to minimize the error and improve its ability to make accurate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cb632c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b035a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "7. What are the steps in the backpropagation algorithm? Why a multi-layer neural network is\n",
    "required?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a8630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The steps in the backpropagation algorithm are as follows:\n",
    "\n",
    "1. Forward Propagation: Pass an input through the network, computing the activations and weighted sums of neurons in each layer, starting from the input layer and moving towards the output layer.\n",
    "\n",
    "2. Error Calculation: Compare the network's output with the desired output using a suitable loss function. Calculate the error or loss, which quantifies the discrepancy between the predicted and desired outputs.\n",
    "\n",
    "3. Backward Propagation: Starting from the output layer, calculate the gradients of the error with respect to the weighted sums of each neuron using the chain rule. These gradients indicate how the error changes with small variations in the weighted sums.\n",
    "\n",
    "4. Weight Update\n",
    "\n",
    ": Use the gradients to update the weights of the connections between neurons. The weight update is typically performed using an optimization algorithm, such as gradient descent or one of its variants. The learning rate determines the step size of weight updates.\n",
    "\n",
    "5. Repeat: Repeat steps 1 to 4 for a batch or mini-batch of training examples. Iterate through multiple epochs, adjusting the weights based on the gradients to reduce the error and improve the network's performance.\n",
    "\n",
    "A multi-layer neural network is required because it allows for the representation of complex, non-linear relationships in the data. The hidden layers introduce non-linear transformations and enable the network to learn and approximate complex functions. In particular, a single-layer perceptron can only learn linearly separable patterns, while a multi-layer perceptron can learn and represent non-linear decision boundaries, making it more suitable for a wider range of tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1313236",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb378e",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Write short notes on:\n",
    "\n",
    "1. Artificial neuron\n",
    "2. Multi-layer perceptron\n",
    "3. Deep learning\n",
    "4. Learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6354fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a) Artificial neuron: An artificial neuron, also called a perceptron, is a fundamental building block of artificial neural networks. It receives inputs, applies weights to them, computes a weighted sum, applies an activation function to the sum, and produces an output. Artificial neurons, inspired by biological neurons, form the basis for information processing and learning in neural networks.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c6154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Multi-layer perceptron (MLP): A multi-layer perceptron is a type of artificial neural network with one or more hidden layers between the input and output layers. Each neuron in the hidden layers applies an activation function to the weighted sum of its inputs. MLPs can learn complex non-linear relationships in data and are widely used for classification, regression, and other tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a900f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Deep learning: Deep learning is a subset of machine learning that focuses on neural networks with multiple layers, allowing the network to learn hierarchical representations of data. Deep learning models, such as deep neural networks (DNNs), convolutional neural networks (CNNs), and recurrent neural networks (RNNs), have achieved breakthroughs in various fields, including computer vision, natural language processing, and speech recognition.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2c9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d) Learning rate: The learning rate is a hyperparameter that determines the step size at which the weights of a neural network are updated during the training process. It controls how much the weights change in response to the calculated gradients. A high learning rate can lead to fast convergence but may cause overshooting or instability. A low learning rate may result in slow convergence or getting stuck in suboptimal solutions. Choosing an appropriate learning rate is crucial for effective training of neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e22ebae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5a5714",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Write the difference between:-\n",
    "\n",
    "1. Activation function vs threshold function\n",
    "2. Step function vs sigmoid function\n",
    "3. Single layer vs multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb1588",
   "metadata": {},
   "outputs": [],
   "source": [
    "a) Activation function vs threshold function:\n",
    "- Activation function: An activation function takes the weighted sum of inputs and produces an output value that determines the activation level of a neuron. Examples include sigmoid, ReLU, and tanh functions. Activation functions introduce non-linearity, allowing neural networks to model complex relationships.\n",
    "- Threshold function: A threshold function is a type of activation function that produces a binary output based on whether the input exceeds a predefined threshold. It returns one value (e.g., 1) if the input is above the threshold and another value (e.g., 0) if the input is below the threshold. Threshold functions are commonly used in simple perceptrons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b126c035",
   "metadata": {},
   "outputs": [],
   "source": [
    "b) Step function vs sigmoid function:\n",
    "- Step function: The step function produces a binary output (e.g., 0 or 1) based on whether the input is greater than or equal to zero. It has a sharp transition at the threshold value and lacks continuity.\n",
    "- Sigmoid function: The sigmoid function maps the input to a continuous output between 0 and 1. It has an S-shaped curve and provides smoothness and differentiability. Sigmoid functions, such as the logistic sigmoid and tanh functions, are commonly used in neural networks as activation functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6241655c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c) Single-layer vs multi-layer perceptron:\n",
    "- Single-layer perceptron: A single-layer perceptron consists of an input layer and an output layer, with no hidden layers. It can learn and represent linear decision boundaries but is limited to solving linearly separable problems.\n",
    "- Multi-layer perceptron: A multi-layer perceptron has one or more hidden layers between the input and output layers. The hidden layers introduce non-linear transformations and enable the network to learn complex non-linear relationships. Multi-layer perceptrons can solve problems that are not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa00c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f64a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5026f3bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cc3cda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0c074",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8bd81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844f53e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c918555c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7bbec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5cd4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89164b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b864e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6c4cac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
