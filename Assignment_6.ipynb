{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acf9c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750b0174",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What are the advantages of a CNN over a fully connected DNN for image classification?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60792c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Advantages of a CNN over a fully connected DNN for image classification include:\n",
    "- Parameter efficiency: CNNs exploit the spatial structure of images by sharing parameters across different regions, resulting in a significantly smaller number of parameters compared to fully connected DNNs.\n",
    "- Translation invariance: CNNs are capable of capturing local patterns and features regardless of their spatial location in an image, making them robust to translations. This property is crucial for tasks such as image classification where the location of the object within the image may vary.\n",
    "- Hierarchical feature learning: CNNs are designed to learn hierarchical representations of images, where lower layers capture low-level features like edges and textures, and higher layers capture more complex features and semantic information. This allows CNNs to learn meaningful and discriminative features for image classification.\n",
    "- Parameter sharing: Due to parameter sharing, CNNs are more adept at generalizing to new images and can better handle variations in object appearance, such as changes in lighting, scale, and pose.\n",
    "- Local receptive fields: CNNs use local receptive fields that scan the image in small regions, enabling them to capture local patterns and exploit local dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c6d754",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160bca3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Consider a CNN composed of three convolutional layers, each with 3 × 3 kernels, a stride of\n",
    "2, and &quot;same&quot; padding. The lowest layer outputs 100 feature maps, the middle one outputs\n",
    "200, and the top one outputs 400. The input images are RGB images of 200 × 300 pixels.\n",
    "What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much\n",
    "RAM will this network require when making a prediction for a single instance? What about when\n",
    "training on a mini-batch of 50 images?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c4681c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total number of parameters in the given CNN:\n",
    "- First convolutional layer: (3 * 3 * 3 + 1) * 100 = 2800 parameters (3 * 3 * 3 represents the kernel size and 100 is the number of output feature maps).\n",
    "- Second convolutional layer: (3 * 3 * 100 + 1) * 200 = 180200 parameters.\n",
    "- Third convolutional layer: (3 * 3 * 200 + 1) * 400 = 720400 parameters.\n",
    "\n",
    "Total parameters = 2800 + 180200 + 720400 = 903400 parameters.\n",
    "\n",
    "To calculate the RAM required for predictions/training:\n",
    "- For a single instance (prediction): RAM = (input image size) * (number of parameters) * (32 bits) = 200 * 300 * 903400 * 32 bits = 6.892 GB (approximately).\n",
    "- For a mini-batch of 50 images (training): RAM = (input image size) * (number of parameters) * (mini-batch size) * (32 bits) = 200 * 300 * 903400 * 50 * 32 bits = 344.6 GB (approximately).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3073b2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e723fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. If your GPU runs out of memory while training a CNN, what are five things you could try to\n",
    "solve the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d996b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "If the GPU runs out of memory while training a CNN, here are five possible solutions:\n",
    "- Reduce batch size: Use a smaller mini-batch size during training, reducing the memory requirement per iteration.\n",
    "- Reduce model size: Decrease the number of parameters in the model by reducing the number of layers, reducing the number of filters per layer, or using smaller kernel sizes.\n",
    "- Use mixed precision: Utilize mixed precision training techniques, such as Tensor Cores on modern GPUs, which allow for using lower-precision floating-point formats (e.g., mixed precision with FP16) to reduce memory usage without sacrificing much accuracy.\n",
    "- Gradient checkpointing: Implement gradient checkpointing techniques to trade off memory usage for additional computation. This method recalculates intermediate activations during backpropagation, reducing the memory needed for storing intermediate gradients.\n",
    "- Data parallelism: If available, distribute the workload across multiple GPUs by using data parallelism, where each GPU processes a different subset of the training data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e61b58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104a0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Why would you want to add a max pooling layer rather than a convolutional layer with the\n",
    "same stride?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7912d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "A max pooling layer is added rather than a convolutional layer with the same stride for several reasons:\n",
    "- Translation invariance: Max pooling layers provide translation invariance by selecting the maximum activation within each pooling region. This helps the network capture the presence of important features in the local neighborhood regardless of their precise location.\n",
    "- Dimensionality reduction: Max pooling reduces the spatial dimensions of the feature maps, which can help control the number of parameters and computational complexity in subsequent layers. It allows for capturing the most salient features while discarding less important details.\n",
    "- Robustness to small spatial shifts: Max pooling can tolerate small translations or shifts in the input, as long as the important features are still preserved. This makes the model more robust to slight variations in object positions or spatial transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f5b0fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f0ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "5. When would you want to add a local response normalization layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e30dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "Local response normalization (LRN) layers were originally introduced to provide local contrast normalization and enhance the generalization capabilities of CNNs. However, they have been largely replaced by batch normalization techniques, which offer better performance and stability during training. Nowadays, batch normalization is preferred over LRN layers in most cases, as it can provide similar benefits along with improved training dynamics and regularization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773667c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5c1645",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Can you name the main innovations in AlexNet, compared to LeNet-5? What about the main\n",
    "innovations in GoogLeNet, ResNet, SENet, and Xception?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508cb027",
   "metadata": {},
   "outputs": [],
   "source": [
    "Innovations in AlexNet compared to LeNet-5:\n",
    "- Deeper architecture: AlexNet had a much deeper architecture compared to LeNet-5, consisting of five convolutional layers and three fully connected layers. This increased depth allowed for learning more complex and hierarchical features.\n",
    "- Rectified Linear Units (ReLU): AlexNet used the ReLU activation function instead of the sigmoid activation function used in LeNet-5. ReLU helped alleviate the vanishing gradient problem, enabling faster and more effective training.\n",
    "- Data augmentation: AlexNet employed data augmentation techniques such as random cropping and horizontal flipping during training, which increased the diversity of training samples and reduced overfitting.\n",
    "- Dropout: AlexNet introduced the concept of dropout, randomly dropping out units during training, to regularize the model and prevent overfitting.\n",
    "\n",
    "Innovations in GoogLeNet:\n",
    "- Inception modules: GoogLeNet introduced the inception module, which combined filters of different sizes and performed parallel convolutions. This allowed the network to capture features at different scales and significantly reduce the number of parameters.\n",
    "- Network depth and width: GoogLeNet utilized a network with significantly more layers and broader architecture, showcasing the benefits of deeper and wider networks.\n",
    "- Auxiliary classifiers: Auxiliary classifiers were added at intermediate layers to encourage the model to learn more discriminative features throughout the network and alleviate the vanishing gradient problem.\n",
    "\n",
    "Innovations in ResNet:\n",
    "- Residual connections: ResNet introduced residual connections, allowing information to bypass certain layers and propagate directly through skip connections. This helped address the vanishing gradient problem and enabled the training of extremely deep networks.\n",
    "- Identity mapping: ResNet emphasized identity mappings in residual blocks, allowing the network to learn incremental changes to the feature maps rather than relearning the complete transformation at each layer.\n",
    "- Shortcut connections: Shortcut connections in ResNet allowed for training deeper networks with improved gradient flow and reduced optimization difficulties.\n",
    "\n",
    "Innovations in SENet:\n",
    "- Squeeze-and-Excitation (SE) blocks: SENet introduced SE blocks, which dynamically recalibrated the feature maps by adaptively weighting channel-wise features based on their importance. This mechanism improved the representational power of the network and enabled better feature discrimination.\n",
    "\n",
    "Innovations in Xception:\n",
    "- Depthwise separable convolutions: Xception utilized depthwise separable convolutions, which split the standard convolution into depthwise and pointwise convolutions. This technique reduced computational complexity and allowed for more efficient learning of spatial and channel-wise feature interactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920e7848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8708220e",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. What is a fully convolutional network? How can you convert a dense layer into a\n",
    "convolutional layer?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e054061",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "A fully convolutional network (FCN) is a type of network that consists solely of convolutional layers, without any fully connected layers. It is commonly used for tasks like semantic segmentation. To convert a dense layer into a convolutional layer, the following steps can be taken:\n",
    "- Replace the dense layer with a 1x1 convolutional layer.\n",
    "- Set the number of filters in the convolutional layer equal to the number of neurons in the dense layer.\n",
    "- Adjust the dimensions of the input to the convolutional layer to match the dimensions of the feature map before the dense layer.\n",
    "- Apply global average pooling or another pooling operation to collapse the spatial dimensions of the feature map before the dense layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68c8f4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830f1ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. What is the main technical difficulty of semantic segmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cba07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The main technical difficulty of semantic segmentation is dealing with the pixel-level prediction of classes in an image. Unlike image classification, where each image is assigned a single label, semantic segmentation requires assigning a label to each pixel in the image. This introduces several challenges:\n",
    "- Spatial context and accuracy: Semantic segmentation requires capturing fine-grained details and preserving spatial relationships between neighboring pixels to accurately assign labels. This can be challenging due to the varying sizes and shapes of objects in the image.\n",
    "- Object occlusion and boundaries: Distinguishing between objects that are close together or partially occluded by other objects is a complex task. Properly segmenting object boundaries is crucial for achieving accurate and visually pleasing results.\n",
    "- Computational complexity: Pixel-wise predictions require processing a large number of pixels in the image, making semantic segmentation computationally expensive. Efficient algorithms and architectures are necessary to achieve real-time or near-real-time performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5090e857",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c59f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. Build your own CNN from scratch and try to achieve the highest possible accuracy on MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6a1afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Building a CNN from scratch to achieve the highest possible accuracy on MNIST would involve defining a suitable architecture, training configuration, and hyperparameters. Here's a high-level overview of the process:\n",
    "- Define the CNN architecture, including convolutional layers, pooling layers, and fully connected layers.\n",
    "- Configure the training process, including the loss function (e.g., cross-entropy), optimizer (e.g., Adam), and any additional metrics.\n",
    "- Preprocess the MNIST dataset, which may involve scaling the pixel values, reshaping the images, and one-hot encoding the labels.\n",
    "- Split the dataset into training and validation sets.\n",
    "- Implement the data pipeline using TensorFlow's Data API or other data loading mechanisms, ensuring efficient loading and preprocessing of the data.\n",
    "- Train the CNN on the training set, monitoring the validation accuracy to prevent overfitting.\n",
    "- Experiment with different hyperparameters, such as learning rate, batch size, and regularization techniques (e.g., dropout), to optimize the model's performance.\n",
    "- Evaluate the trained model on the test set to measure its accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ddcfee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303efc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42f2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "10. Use transfer learning for large image classification, going through these steps:\n",
    "a. Create a training set containing at least 100 images per class. For example, you could\n",
    "classify your own pictures based on the location (beach, mountain, city, etc.), or\n",
    "alternatively you can use an existing dataset (e.g., from TensorFlow Datasets).\n",
    "b. Split it into a training set, a validation set, and a test set.\n",
    "c. Build the input pipeline, including the appropriate preprocessing operations, and\n",
    "optionally add data augmentation.\n",
    "d. Fine-tune a pretrained model on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44c42c9",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfef7ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788e7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d1f9a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1476f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86e8d5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff78a19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6140438",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58296b23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419bdd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
