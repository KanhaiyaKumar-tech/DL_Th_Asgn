{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ff5cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b05b66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Why is it generally preferable to use a Logistic Regression classifier rather than a classical\n",
    "Perceptron (i.e., a single layer of linear threshold units trained using the Perceptron training\n",
    "algorithm)? How can you tweak a Perceptron to make it equivalent to a Logistic Regression\n",
    "classifier?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00008136",
   "metadata": {},
   "outputs": [],
   "source": [
    "Logistic Regression is generally preferable to a classical Perceptron because it provides probabilistic outputs and can handle linearly inseparable data. The Perceptron, with its binary step activation function, can only classify linearly separable data. To make a Perceptron equivalent to a Logistic Regression classifier, we can replace the step activation function with a logistic (sigmoid) activation function. This modification allows the Perceptron to output probabilities and perform binary classification using logistic regression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f90ba3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfbc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Why was the logistic activation function a key ingredient in training the first MLPs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ade193",
   "metadata": {},
   "outputs": [],
   "source": [
    "The logistic activation function (also known as the sigmoid function) was a key ingredient in training the first Multilayer Perceptrons (MLPs) because of its differentiability and the convenient mathematical properties it provides. The logistic function ensures that the output of each neuron is bounded between 0 and 1, allowing for the interpretation of the output as probabilities. Additionally, the logistic function has a smooth gradient, enabling efficient gradient-based optimization algorithms, such as backpropagation, to be used for training MLPs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e83eec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe7af6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Name three popular activation functions. Can you draw them?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dfc107",
   "metadata": {},
   "outputs": [],
   "source": [
    "Three popular activation functions are:\n",
    "   - Sigmoid (Logistic) function:\n",
    "   \n",
    "   - Rectified Linear Unit (ReLU) function:\n",
    "    \n",
    "   - Hyperbolic tangent (tanh) function:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81be0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bee3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Suppose you have an MLP composed of one input layer with 10 passthrough neurons,\n",
    "followed by one hidden layer with 50 artificial neurons, and finally one output layer with 3\n",
    "artificial neurons. All artificial neurons use the ReLU activation function.\n",
    "a) What is the shape of the input matrix X?\n",
    "b) What about the shape of the hidden layer’s weight vector Wh, and the shape of its\n",
    "bias vector bh?\n",
    "c) What is the shape of the output layer’s weight vector Wo, and its bias vector bo?\n",
    "d) What is the shape of the network’s output matrix Y?\n",
    "e) Write the equation that computes the network’s output matrix Y as a function\n",
    "of X, Wh, bh, Wo and bo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6fc124",
   "metadata": {},
   "outputs": [],
   "source": [
    " For an MLP with one input layer, one hidden layer, and one output layer:\n",
    "   - The shape of the input matrix X will be (batch_size, num_features), where batch_size is the number of samples in the input and num_features is the number of features.\n",
    "   - The shape of the hidden layer's weight vector Wh will be (num_features, num_hidden_neurons), where num_hidden_neurons is the number of artificial neurons in the hidden layer.\n",
    "   - The shape of the hidden layer's bias vector bh will be (num_hidden_neurons,).\n",
    "   - The shape of the output layer's weight vector Wo will be (num_hidden_neurons, num_output_neurons), where num_output_neurons is the number of artificial neurons in the output layer.\n",
    "   - The shape of the output layer's bias vector bo will be (num_output_neurons,).\n",
    "   - The shape of the network's output matrix Y will be (batch_size, num_output_neurons).\n",
    "   - The equation that computes the network's output matrix Y as a function of X, Wh, bh, Wo, and bo is:\n",
    "     Y = ReLU(X.dot(Wh) + bh).dot(Wo) + bo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5971c48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be05d00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75a0a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7e36e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6714db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31793ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. How many neurons do you need in the output layer if you want to classify email into spam\n",
    "or ham? What activation function should you use in the output layer? If instead you want to\n",
    "tackle MNIST, how many neurons do you need in the output layer, using what activation\n",
    "function?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6cdaa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "If you want to classify email into spam or ham, you would need only 1 neuron in the output layer. The activation function in the output layer should be the sigmoid function to produce a binary classification output between 0 and 1.\n",
    "   If you want to tackle the MNIST dataset, you would need 10 neurons in the output layer, one for each digit (0 to 9). The activation function in the output layer should be the softmax function, which normalizes the outputs to represent the probabilities of each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7006a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cca6254",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What is backpropagation and how does it work? What is the difference between\n",
    "backpropagation and reverse-mode autodiff?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd6d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Backpropagation is a training algorithm used to update the weights of a neural network by propagating the gradients of the loss function with respect to the network's parameters backward through the network. It works by calculating the gradients of the loss function with respect to the output of each layer, and then applying the chain rule to propagate these gradients backward layer by layer. The gradients are used to update the weights through an optimization algorithm such as stochastic gradient descent (SGD).\n",
    "\n",
    "Reverse-mode automatic differentiation (autodiff) is a technique used to efficiently compute gradients in deep neural networks. Backpropagation is a specific implementation of reverse-mode autodiff, tailored for training neural networks. While both backpropagation and reverse-mode autodiff involve propagating gradients backward through the network, reverse-mode autodiff is a more general concept that can be applied to compute gradients in various computational graphs, not just neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc0d66f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e78e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Can you list all the hyperparameters you can tweak in an MLP? If the MLP overfits the\n",
    "training data, how could you tweak these hyperparameters to try to solve the problem?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c04bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "The hyperparameters that can be tweaked in an MLP include:\n",
    "   - Number of hidden layers and the number of neurons in each hidden layer.\n",
    "   - Choice of activation functions for each layer.\n",
    "   - Learning rate and optimization algorithm (e.g., SGD, Adam, RMSprop).\n",
    "   - Regularization techniques such as L1 or L2 regularization.\n",
    "   - Dropout rate, if using dropout regularization.\n",
    "   - Batch size and number of training epochs.\n",
    "   - Initialization strategy for weights and biases.\n",
    "   - Learning rate schedule.\n",
    "   - Early stopping criteria.\n",
    "\n",
    "If the MLP overfits the training data, some possible ways to address the problem include:\n",
    "   - Decreasing the number of hidden neurons or layers to reduce model complexity.\n",
    "   - Applying regularization techniques such as L1 or L2 regularization to penalize large weights.\n",
    "   - Increasing the dropout rate to introduce more regularization.\n",
    "   - Collecting more training data if possible.\n",
    "   - Adjusting the learning rate or using learning rate schedules to control the rate of learning.\n",
    "   - Applying early stopping to stop training when the validation loss stops improving.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85866563",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7811676",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf9978c",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. Train a deep MLP on the MNIST dataset and see if you can get over 98% precision. Try\n",
    "adding all the bells and whistles (i.e., save checkpoints, restore the last checkpoint in case of\n",
    "an interruption, add summaries, plot learning curves using TensorBoard, and so on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7be9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c86800",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16df6159",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc0939",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90120c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bd66fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
